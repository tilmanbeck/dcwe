{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d4aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, \\\n",
    "    IntervalStrategy, EarlyStoppingCallback, AutoConfig, EvalPrediction\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98006163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(data, label_map):\n",
    "    data['label'] = [label_map[i] for i in data['label']]\n",
    "    return data\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    if len(set(preds))==2:\n",
    "        f1_bin = f1_score(p.label_ids, preds, average='binary')\n",
    "    else:\n",
    "        f1_bin = -1\n",
    "    result = {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item(),\n",
    "              'f1': f1_bin,\n",
    "              'f1_macro': f1_score(p.label_ids, preds, average='macro'),\n",
    "              'f1_micro': f1_score(p.label_ids, preds, average='micro')}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c30d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0010c5ce28>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables\n",
    "\n",
    "label_maps = {\n",
    "    'debate': {'claim': 1, 'noclaim': 0},\n",
    "    'sandy': {'y': 1, 'n': 0},\n",
    "    'rumours':  {'comment': 2, 'deny': 1, 'support': 3, 'query': 0},\n",
    "    'clex': {'Related - but not informative': 2, 'Not related': 1,\n",
    "             'Related and informative': 3, 'Not applicable': 0}\n",
    "}\n",
    "\n",
    "label_maps_inverse = {\n",
    "    'debate': {1: 'claim', 0: 'noclaim'},\n",
    "    'sandy': {1: 'y', 0:'n'},\n",
    "    'rumours':  {0: 'query', 1: 'deny', 2:'comment', 3:'support'},\n",
    "    'clex': {2: 'Related - but not informative', 1: 'Not related',\n",
    "             3: 'Related and informative', 0:'Not applicable'}\n",
    "}\n",
    "\n",
    "data_name = 'sandy'\n",
    "data_dir = '/ukp-storage-1/beck/Repositories/temporal-adaptation/datasets/stowe-2018/labeled/stowe-2018-labeled-all.csv'\n",
    "partition = 'time_stratified_partition'\n",
    "output_dir = '/ukp-storage-1/beck/Repositories/dcwe/results/jupyter'\n",
    "label_map = label_maps[data_name]\n",
    "inverse_label_map = label_maps_inverse[data_name]\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "n_epochs = 2\n",
    "warmup_ratio =0.1\n",
    "weight_decay=0.01\n",
    "modelname = 'bert-base-cased'\n",
    "time_field = 'date'\n",
    "label_field = 'tag'\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c061233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5f129a02744c82baa5f3637b91c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555adfc5a7324440868b2797e1a0e239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091d97fd51a3421c9653fb0f36a2ff2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(data_dir, encoding='utf-8')\n",
    "n_labels = len(set(dataframe[label_field].values))\n",
    "# take the corresponding split (if it exists otherwise take random split)\n",
    "\n",
    "# rename data columns to common format\n",
    "dataframe.rename(columns={label_field: 'label', time_field: 'time'}, inplace=True)\n",
    "# convert string labels to numeric\n",
    "dataframe['label'] = dataframe['label'].replace(label_map)\n",
    "\n",
    "dataframe.dropna(subset=[partition, 'label', 'time'], inplace=True)\n",
    "dataframe.time = pd.to_datetime(dataframe.time)\n",
    "dataframe.reset_index(inplace=True, drop=True)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_dir = output_dir\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "# load split data\n",
    "train_data = dataframe[dataframe[partition] == \"train\"]\n",
    "train_data = pd.DataFrame(dataframe.iloc[train_data.index])\n",
    "train_dataset = Dataset.from_pandas(train_data).map(\n",
    "    lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length'), batched=True)\n",
    "\n",
    "validation_data = dataframe[dataframe[partition] == \"dev\"]\n",
    "validation_data = pd.DataFrame(dataframe.iloc[validation_data.index])\n",
    "validation_dataset = Dataset.from_pandas(validation_data).map(\n",
    "    lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length'), batched=True)\n",
    "\n",
    "test_data = dataframe[dataframe[partition] == \"test\"]\n",
    "test_data = pd.DataFrame(dataframe.iloc[test_data.index])\n",
    "test_dataset = Dataset.from_pandas(test_data).map(\n",
    "    lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc252af7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /ukp-storage-1/beck/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model preparation\n",
    "def model_init():\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        modelname,\n",
    "        num_labels=n_labels\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(modelname, config=config)\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,          # output directory\n",
    "    num_train_epochs=n_epochs,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_ratio=warmup_ratio,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=weight_decay,               # strength of weight decay\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    save_total_limit=1,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_init(),                   # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=validation_dataset,      # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff00e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: time_stratified_partition, id, tweet_id, progressive_bin, __index_level_0__, controlled_partition, text, time.\n",
      "***** Running training *****\n",
      "  Num examples = 10358\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1296' max='1296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1296/1296 20:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Bin</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.270800</td>\n",
       "      <td>0.290252</td>\n",
       "      <td>0.933102</td>\n",
       "      <td>0.830022</td>\n",
       "      <td>0.894189</td>\n",
       "      <td>0.933102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.205263</td>\n",
       "      <td>0.937446</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.909576</td>\n",
       "      <td>0.937446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: time_stratified_partition, id, tweet_id, progressive_bin, __index_level_0__, controlled_partition, text, time.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1151\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-648\n",
      "Configuration saved in /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-648/config.json\n",
      "Model weights saved in /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-648/pytorch_model.bin\n",
      "Deleting older checkpoint [/ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-1296] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: time_stratified_partition, id, tweet_id, progressive_bin, __index_level_0__, controlled_partition, text, time.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1151\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-1296\n",
      "Configuration saved in /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-1296/config.json\n",
      "Model weights saved in /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-1296/pytorch_model.bin\n",
      "Deleting older checkpoint [/ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-648] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /ukp-storage-1/beck/Repositories/dcwe/results/jupyter/checkpoint-1296 (score: 0.9095757681564246).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: time_stratified_partition, id, tweet_id, progressive_bin, __index_level_0__, controlled_partition, text, time.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1151\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: time_stratified_partition, id, tweet_id, progressive_bin, __index_level_0__, controlled_partition, text, time.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5754\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8dc1ee143069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0minverse_label_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-8dc1ee143069>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0minverse_label_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "test_results = trainer.predict(test_dataset=test_dataset)\n",
    "preds = test_results.predictions[0] if isinstance(test_results.predictions, tuple) else test_results.predictions\n",
    "preds =  [inverse_label_map[i] for i in list(np.argmax(preds, axis=1))]\n",
    "truth =  [label_map[i] for i in list(test_results.label_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992b2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test_results.predictions[0] if isinstance(test_results.predictions, tuple) else test_results.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49ebc793",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_label_map = label_maps_inverse[data_name]\n",
    "preds =  [inverse_label_map[i] for i in list(np.argmax(preds, axis=1))]\n",
    "truth =  [inverse_label_map[i] for i in list(test_results.label_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0d5d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8291280740977323"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(truth, preds, pos_label='y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
